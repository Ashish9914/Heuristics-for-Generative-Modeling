{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Popular GAN Hacks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-24WbVC4VUO",
        "colab_type": "text"
      },
      "source": [
        "At the time of writing, there is no good theoretical foundation as to how to design and train GAN models, but there is established literature of heuristics, or “hacks,” that have been empirically demonstrated to work well in practice.\n",
        "\n",
        "\n",
        "1.Downsample using strided convolutions\n",
        "\n",
        "Normally in deep convolutional neural networks we use pooling layers to downsample the input and feature maps with the depth of the networks but in DCGAN, for the implementation of discriminator network we don't folllow the above mentioned method we use downampling with strided convolutions , we change the dimensions of the stride from (1,1) to (2,2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8tZexjK4j3M",
        "colab_type": "code",
        "outputId": "24e953b4-f161-4105-e552-1aca4892477a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64,kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "=================================================================\n",
            "Total params: 1,792\n",
            "Trainable params: 1,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ThnuKHX56LG",
        "colab_type": "text"
      },
      "source": [
        "2.Unsample using strides\n",
        "\n",
        "The generator model must generate an output image given as input at a random point from the latent space.\n",
        "\n",
        "The recommended approach for achieving this is to use a transpose convolutional layer with a strided convolution. This is a special type of layer that performs the convolution operation in reverse. Intuitively, this means that setting a stride of 2×2 will have the opposite effect, upsampling the input instead of downsampling it in the case of a normal convolutional layer.\n",
        "\n",
        "By stacking a transpose convolutional layer with strided convolutions, the generator model is able to scale a given input to the desired output dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjGAIH936Gdd",
        "colab_type": "code",
        "outputId": "fec59a67-6145-47a6-e036-cae37ea8fa0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same', input_shape=(64,64,3)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64)      3136      \n",
            "=================================================================\n",
            "Total params: 3,136\n",
            "Trainable params: 3,136\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ya6m_Gm6R_o",
        "colab_type": "text"
      },
      "source": [
        "3.Use Leaky Relu\n",
        "\n",
        "The rectified linear activation unit, or ReLU for short, is a simple calculation that returns the value provided as input directly, or the value 0.0 if the input is 0.0 or less.The best practice for GANs is to use a variation of the ReLU that allows some values less than zero and learns where the cut-off should be in each node. This is called the leaky rectified linear activation unit, or LeakyReLU for short.\n",
        "\n",
        "A negative slope can be specified for the LeakyReLU and the default value of 0.2 is recommended.\n",
        "\n",
        "Originally, ReLU was recommend for use in the generator model and LeakyReLU was recommended for use in the discriminator model, although more recently, the LeakyReLU is recommended in both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrQ4KrmO6hWX",
        "colab_type": "code",
        "outputId": "d3650983-146c-472c-927d-a3768ea57e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3)))\n",
        "model.add(LeakyReLU(0.2))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
            "=================================================================\n",
            "Total params: 1,792\n",
            "Trainable params: 1,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyFuZMQC6pm9",
        "colab_type": "text"
      },
      "source": [
        "4.Batch normalisation\n",
        "\n",
        "Batch normalization is used after the activation of convolution and transpose convolutional layers in the discriminator and generator models respectively.\n",
        "\n",
        "It is added to the model after the hidden layer, but before the activation, such as LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L19S_OYe64Yb",
        "colab_type": "code",
        "outputId": "839e921f-9131-490c-fa9a-971ac3933471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(0.2))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
            "=================================================================\n",
            "Total params: 2,048\n",
            "Trainable params: 1,920\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ZIrT4t7A5L",
        "colab_type": "text"
      },
      "source": [
        "5.Use Guassian weight initialization\n",
        "\n",
        "The best practice for DCAGAN models reported in the paper is to initialize all weights using a zero-centered Gaussian distribution (the normal or bell-shaped distribution) with a standard deviation of 0.02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW3Y7ebm7K2m",
        "colab_type": "code",
        "outputId": "a3aa27cf-1bad-4fb8-e2e7-be86868beb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "model = Sequential()\n",
        "init = RandomNormal(mean=0.0, stddev=0.02)\n",
        "model.add(Conv2DTranspose(64, kernel_size=(4,4), strides=(2,2), padding='same', kernel_initializer=init, input_shape=(64,64,3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wASFS9FJ7T99",
        "colab_type": "text"
      },
      "source": [
        "6. Use Adam stochastic gradient descent\n",
        "\n",
        "There are many variants of the training algorithm. The best practice for training DCGAN models is to use the Adam version of stochastic gradient descent with the learning rate of 0.0002 and the beta1 momentum value of 0.5 instead of the default of 0.9.\n",
        "\n",
        "The Adam optimization algorithm with this configuration is recommended when both optimizing the discriminator and generator models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wqY8-N-7lmq",
        "colab_type": "code",
        "outputId": "af31ca5d-96df-4346-a2dc-baa70266b34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2), padding='same', input_shape=(64,64,3)))\n",
        "\n",
        "opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2v0lz4T7v_j",
        "colab_type": "text"
      },
      "source": [
        "7.Scale images to range[-1,1]\n",
        "\n",
        "t is recommended to use the hyperbolic tangent activation function as the output from the generator model.\n",
        "\n",
        "As such, it is also recommended that real images used to train the discriminator are scaled so that their pixel values are in the range [-1,1]. This is so that the discriminator will always receive images as input, real and fake, that have pixel values in the same range.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1Z7AA9R76PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example of a function for scaling images\n",
        " \n",
        "# scale image data from [0,255] to [-1,1]\n",
        "def scale_images(images):\n",
        "\t# convert from unit8 to float32\n",
        "\timages = images.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\timages = (images - 127.5) / 127.5\n",
        "\treturn images"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}